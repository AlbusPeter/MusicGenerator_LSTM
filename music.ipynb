{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('./data/input.txt','r')\n",
    "data = list(data)\n",
    "data_use = []\n",
    "data_f = []\n",
    "index = 0\n",
    "temp = ''\n",
    "for d in data:\n",
    "    if d == \"<start>\\r\\n\":\n",
    "        index = 1\n",
    "        continue\n",
    "    if d == '<end>\\r\\n':\n",
    "        index = 0\n",
    "    if index == 1:\n",
    "        temp = temp + str(d)\n",
    "    if index == 0:\n",
    "        data_use.append(temp)\n",
    "        temp = ''\n",
    "#print(d)\n",
    "for d in data_use:\n",
    "    if len(d) > 40:\n",
    "        data_f.append(d)\n",
    "data_use = data_f\n",
    "all_character = set()\n",
    "counter = 0\n",
    "for d in data_use:\n",
    "    for dd in d:\n",
    "        all_character.add(dd)\n",
    "all_character = list(all_character)\n",
    "nletter = len(all_character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_use[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line),nletter)\n",
    "    position = np.zeros(len(line))\n",
    "    for i, letter in enumerate(line):\n",
    "        position[i] = all_character.index(letter)\n",
    "        tensor[i][all_character.index(letter)] = 1\n",
    "    position = torch.LongTensor(position[1:])\n",
    "    return autograd.Variable(tensor[:-1,:]).cuda(),autograd.Variable(position).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_g(data,batch_size):\n",
    "    lyric = np.random.randint(len(data))\n",
    "    #print(len(data[lyric]))\n",
    "    if len(data[lyric])-batch_size >0:\n",
    "        interval_s = np.random.randint(len(data[lyric])-batch_size)\n",
    "        return data[lyric][interval_s:interval_s+batch_size]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,letter_dim,hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.letter_dim = letter_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        self.lstm = nn.LSTM(letter_dim,hidden_dim)\n",
    "        self.hidden2out = nn.Linear(hidden_dim,letter_dim)\n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)).cuda(),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)).cuda())\n",
    "    def forward(self,line):\n",
    "        lstm_out, self.hidden = self.lstm(line.view(len(line),1,-1), self.hidden)\n",
    "        out_space = self.hidden2out(lstm_out.view(len(lstm_out),-1))\n",
    "#         out_scores = F.softmax(out_space / 1.0,dim = 1)\n",
    "        return out_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 100\n",
    "batch_size = 30\n",
    "lstm = LSTM(nletter,hidden_dim).cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100000):\n",
    "    lstm.zero_grad()\n",
    "    batch = batch_g(data_use,batch_size)\n",
    "#     print(batch)\n",
    "#     print('\\n')\n",
    "    if len(batch)>0:\n",
    "        batch_in,batch_t = lineToTensor(batch)\n",
    "        lstm.hidden = lstm.init_hidden()\n",
    "        out_scores = lstm(batch_in)\n",
    "        loss = loss_function(out_scores,batch_t)\n",
    "        if epoch % 10000 == 0:\n",
    "            print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F.softmax(out_scores / 1.0,dim = 1).data.cpu().numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present = torch.zeros(1,nletter)\n",
    "present[0][all_character.index('X')] = 1\n",
    "present = autograd.Variable(present).cuda()\n",
    "lyric = []\n",
    "lyric.append(all_character[int(torch.distributions.Categorical(F.softmax(lstm(present)/1.0,dim = 1).data.cpu()).sample().numpy())])\n",
    "for num_generation in range(400):\n",
    "    #print(lstm.forward(present).cpu())\n",
    "    next_char_index = int(torch.distributions.Categorical(F.softmax(lstm(present)/1.0,dim = 1).data.cpu()).sample().numpy())\n",
    "    next_char = all_character[next_char_index]\n",
    "    lyric.append(next_char)\n",
    "    present = torch.zeros(1,nletter)\n",
    "    present[0][all_character.index(next_char)] = 1\n",
    "    present = autograd.Variable(present).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(lyric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Music = \"\"\n",
    "for char in lyric:\n",
    "    Music = Music + char\n",
    "print(Music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
