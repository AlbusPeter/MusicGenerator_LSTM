{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('./data/input.txt','r')\n",
    "data = list(data)\n",
    "data_use = []\n",
    "data_f = []\n",
    "index = 0\n",
    "temp = ''\n",
    "for d in data:\n",
    "    if d == \"<start>\\r\\n\":\n",
    "        index = 1\n",
    "        continue\n",
    "    if d == '<end>\\r\\n':\n",
    "        index = 0\n",
    "    if index == 1:\n",
    "        temp = temp + str(d)\n",
    "    if index == 0:\n",
    "        data_use.append(temp)\n",
    "        temp = ''\n",
    "#print(d)\n",
    "for d in data_use:\n",
    "    if len(d) > 40:\n",
    "        data_f.append(d)\n",
    "data_use = data_f\n",
    "all_character = set()\n",
    "counter = 0\n",
    "for d in data_use:\n",
    "    for dd in d:\n",
    "        all_character.add(dd)\n",
    "all_character = list(all_character)\n",
    "nletter = len(all_character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XIV?me si?cle)\r\n",
      "Z:Transcrit et/ou corrig? par Michel BELLON - 2005-03-27\r\n",
      "Z:Pour toute observation mailto:galouvielle@free.fr\r\n",
      "M:2/2\r\n",
      "L:1/4\r\n",
      "Q:1/4=110\r\n",
      "K:Fmaj\r\n",
      "V:1\r\n",
      "dd/2c/2 dB| GB c2|_e/2d/2B/2c/2 AG|Bc c/2B/2 G|F4|dd/2c/2 dB| GB c2|_e/2d/2B/2c/2 AG|Bc c/2B/2 G/2A/4F/4|G2 zG|\r\n",
      "w:Stel-__la splen-dens in mon-____te ut so-lis ra-_di-um Mi-__ra-cu-lis ser-ra-____to ex-au-di po-_pu-__lum  Prin-\r\n",
      "Gd eg|ef/2d/2 ce|fd c/2B/2B/2A/2|G2 zG|Gd eg|ef/2d/2 ce|fd c/2B/2B/2A/2|G2 z2||\r\n",
      "w:ci-pes et ma-gna-__ tes ex-stir-pe re-_gi-_a sae-cu-li po-tes-ta-__tes ob-ten-ta ve-_ni-_a\r\n",
      "dd/2c/2 dB| GB c2|_e/2d/2B/2c/2 AG|Bc c/2B/2 G|F4|dd/2c/2 dB| GB c2|_e/2d/2B/2c/2 AG|Bc c/2B/2 G/2A/4F/4|G4|]\r\n",
      "w:Pe-__cca mi num pro-cla-____mant tu den tes pec-_to-ra Po-__pli te fle-xo cla-____mant hic A-ve Ma-_ri-__a\r\n",
      "V:2\r\n",
      "G2 GB|dG F2|B2 cd|GF cd|f4| G2 GB|dG F2|B2 cd|GF Bc|d2 zd-|\r\n",
      "d G2 G|cB A2|FG Bc|d2 zd-|d G2 G|cB A2|FG Bc|d2 z2|\r\n",
      "G2 GB|dG F2|B2 cd|GF cd|f4| G2 GB|dG F2|B2 cd|GF Bc|d4|]\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_use[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line),nletter)\n",
    "    position = np.zeros(len(line))\n",
    "    for i, letter in enumerate(line):\n",
    "        position[i] = all_character.index(letter)\n",
    "        tensor[i][all_character.index(letter)] = 1\n",
    "    position = torch.LongTensor(position[1:])\n",
    "    return autograd.Variable(tensor[:-1,:]),autograd.Variable(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_g(data,batch_size):\n",
    "    lyric = np.random.randint(len(data))\n",
    "    #print(len(data[lyric]))\n",
    "    if len(data[lyric])-batch_size >0:\n",
    "        interval_s = np.random.randint(len(data[lyric])-batch_size)\n",
    "    return data[lyric][interval_s:interval_s+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,letter_dim,hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.letter_dim = letter_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        self.lstm = nn.LSTM(letter_dim,hidden_dim)\n",
    "        self.hidden2out = nn.Linear(hidden_dim,letter_dim)\n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "    def forward(self,line):\n",
    "        lstm_out, self.hidden = self.lstm(line.view(len(line),1,-1), self.hidden)\n",
    "        out_space = self.hidden2out(lstm_out.view(len(lstm_out),-1))\n",
    "        out_scores = F.softmax(out_space / 1.0,dim = 1)\n",
    "        return out_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 100\n",
    "batch_size = 50\n",
    "lstm = LSTM(nletter,hidden_dim)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch_g(data_use,batch_size)\n",
    "test = lineToTensor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5434\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 4.5432\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 4.5433\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 4.5433\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 4.5436\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 4.5434\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 4.5432\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 4.5432\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 4.5433\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 4.5433\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    lstm.zero_grad()\n",
    "    lstm.hidden = lstm.init_hidden()\n",
    "    batch = batch_g(data_use,batch_size)\n",
    "#     print(batch)\n",
    "#     print('\\n')\n",
    "    batch_in,batch_t = lineToTensor(batch)\n",
    "    lstm.hidden = lstm.init_hidden()\n",
    "    out_scores = lstm(batch_in)\n",
    "    loss = loss_function(out_scores,batch_t)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01075433  0.01105179  0.01077969  0.01003794  0.01034885  0.01041456\n",
      "  0.00991008  0.01134106  0.01018434  0.01121971  0.01075973  0.00962648\n",
      "  0.01015214  0.01150872  0.01173328  0.0111233   0.01027136  0.01004249\n",
      "  0.01041565  0.01098577  0.00951361  0.01066574  0.01093129  0.01098913\n",
      "  0.01077896  0.01005479  0.00950235  0.01012232  0.01035293  0.01019939\n",
      "  0.01130444  0.01103503  0.01010009  0.0100123   0.01071326  0.01044603\n",
      "  0.01099993  0.01170065  0.01058187  0.01107686  0.01169912  0.01114512\n",
      "  0.01071014  0.01009736  0.00992633  0.01163067  0.01124649  0.00964411\n",
      "  0.01107274  0.01077758  0.01000686  0.01113962  0.01044772  0.00985584\n",
      "  0.01043889  0.01042169  0.01105391  0.01090419  0.01015606  0.0112872\n",
      "  0.0109391   0.00972183  0.01069941  0.00957665  0.01152977  0.011324\n",
      "  0.01088302  0.01130027  0.01064119  0.01153235  0.01139959  0.01023003\n",
      "  0.01021276  0.01046022  0.01079297  0.01008717  0.01114451  0.01059628\n",
      "  0.01020307  0.01007797  0.01076368  0.01110929  0.00939706  0.01062294\n",
      "  0.01070457  0.0109523   0.01014319  0.00987528  0.00967446  0.0111376\n",
      "  0.01051205  0.01145473  0.0117924   0.01110441]\n"
     ]
    }
   ],
   "source": [
    "print(out_scores.data.numpy()[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#m = torch.distributions.Categorical(lstm(present).data)\n",
    "print(torch.distributions.Categorical(lstm(present).data).sample().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "present = torch.zeros(1,nletter)\n",
    "present[0][all_character.index('X')] = 1\n",
    "present = autograd.Variable(present)\n",
    "lyric = []\n",
    "lyric.append(all_character[int(torch.distributions.Categorical(lstm(present).data).sample().numpy())])\n",
    "for num_generation in range(200):\n",
    "    #print(lstm.forward(present).cpu())\n",
    "    next_char_index = int(torch.distributions.Categorical(lstm(present).data).sample().numpy())\n",
    "    next_char = all_character[next_char_index]\n",
    "    lyric.append(next_char)\n",
    "    present = torch.zeros(1,nletter)\n",
    "    present[0][all_character.index(next_char)] = 1\n",
    "    present = autograd.Variable(present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'N', 'P', ':', 'i', 'm', '!', '<', 'D', '5', 'F', 'F', 'i', ',', 'L', 'Y', 'd', 'a', 'J', 'Y', 'w', 'P', '3', 'F', 'U', ':', '=', 'U', ' ', 'S', 'N', '&', 'n', 'P', 'J', 'r', '.', 'Z', '\\n', 't', '[', 'K', '8', '\\t', '_', 'e', '\\n', 'v', '+', '|', 'V', 'j', '^', '(', 'P', ')', 'h', 'j', 'b', '[', 'R', 'q', 'G', 'L', 'S', 'S', 'b', 'd', '!', 'o', 'D', 'l', 'F', 's', 'W', '3', 'I', '#', '}', 't', 'h', '_', 'h', '0', '~', '.', 'Q', 'B', 'c', 'D', '@', 'E', 'C', 'D', 'S', ':', 'C', 'x', 'E', 'd', 'G', '/', 'O', 'M', ' ', \"'\", 'T', '!', 'E', 'i', 'C', 'O', '\\r', '*', 'l', 'V', '#', 'i', 'J', 'A', 'W', 'D', 't', '0', 'R', 'J', '?', 'H', 'V', 'h', 'w', '3', 'f', '!', '8', '=', 'b', '?', 'z', 'Z', 'j', '[', 'I', 'z', '=', 'D', 'b', '^', 'k', '(', 'm', 'L', 'h', 'X', 'B', 'b', '\\\\', '\\r', 'O', 'P', '!', '5', 'J', 'X', 'R', 'n', '0', 'Z', '2', 'k', 'o', 'c', '-', 'w', 'v', '4', 'i', 'E', 'K', ']', 'X', 'v', 't', 'O', '\\n', '9', 'k', '+', 'C', '7', ':', '*', 'x', 't', 'B', 'q', 'H', '8', 'B', 'M', 'I']\n"
     ]
    }
   ],
   "source": [
    "print(str(lyric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNP:im!<D5FFi,LYdaJYwP3FU:=U SN&nPJr.Z\n",
      "t[K8\t_e\n",
      "v+|Vj^(P)hjb[RqGLSSbd!oDlFsW3I#}th_h0~.QBcD@ECDS:CxEdG/OM 'T!EiCO\r",
      "*lV#iJAWDt0RJ?HVhw3f!8=b?zZj[Iz=Db^k(mLhXBb\\\r",
      "OP!5JXRn0Z2koc-wv4iEK]XvtO\n",
      "9k+C7:*xtBqH8BMI\n"
     ]
    }
   ],
   "source": [
    "Music = \"\"\n",
    "for char in lyric:\n",
    "    Music = Music + char\n",
    "print(Music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
